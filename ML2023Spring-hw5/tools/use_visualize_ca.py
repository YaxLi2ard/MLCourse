import sys
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
from tokenize_ import Tokenizer
sys.path.append('../')
from model.Transformer import Transformer
import numpy as np


def main():
    # è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # ------------------ Step 1: åŠ è½½æ¨¡å‹å’Œ tokenizer ------------------
    # tokenizer
    tokenizer_src = Tokenizer("ted2020.model")
    tokenizer_tgt = Tokenizer("ted2020.model")  # è‹±æ–‡å’Œä¸­æ–‡ç”¨åŒä¸€æ¨¡å‹
    # æ¨¡å‹
    model = Transformer(src_vocab_size=tokenizer_src.vocab_size(), tgt_vocab_size=tokenizer_tgt.vocab_size())
    model.to(device)

    model.load_state_dict(torch.load("../cpt/transformer_25.436.pt", map_location=device), strict=False)
    model.eval()
    print('æ¨¡å‹åŠ è½½å®Œæˆ...')

    # ------------------ Step 2: è¾“å…¥æ–‡æœ¬ç¼–ç  ------------------
    text = """ After we finished our lunch, we went to the park and played there for more than two hours. """  # He'll do sports tomorrow if the weather's good.
    src_ids = tokenizer_src.encode(text)  # åŠ ä¸Š <bos> å’Œ <eos>
    src_tensor = torch.tensor([src_ids], dtype=torch.long).to(device)

    src_mask = (src_tensor == tokenizer_src.pad_id())

    # ------------------ Step 3: ç¼–ç å™¨å‰å‘ ------------------
    memory = model.encoder(src_tensor, attention_mask=~src_mask)
    print('ç¼–ç å™¨å‰å‘å®Œæˆ...')

    # ------------------ Step 4: è§£ç å™¨é€æ­¥ç”Ÿæˆå¹¶è®°å½•æ³¨æ„åŠ› ------------------
    ys = torch.tensor([[tokenizer_tgt.bos_id()]], dtype=torch.long).to(device)
    output_tokens = []
    attention_maps = []

    for _ in range(101):
        tgt_mask = torch.triu(torch.ones((ys.size(1), ys.size(1)), device=device), diagonal=1).bool()

        # forward with output attention
        out = model.decoder(
            input_ids=ys,
            encoder_hidden_states=memory.last_hidden_state,
            encoder_attention_mask=~src_mask,
            output_attentions=True,  # è·å–æ³¨æ„åŠ›
            return_dict=True
        )
        # è·å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥é¢„æµ‹
        logits = model.output_fc(out.last_hidden_state[:, -1])
        next_token = logits.argmax(dim=-1).item()
        output_tokens.append(next_token)
        ys = torch.cat([ys, torch.tensor([[next_token]], device=device)], dim=1)

        # ä¿å­˜æœ€åä¸€å±‚äº¤å‰æ³¨æ„åŠ›ï¼ˆdecoder æœ€åä¸€å±‚ï¼‰
        # shape: [batch, num_heads, tgt_len, src_len]
        cross_attn = out.cross_attentions[1][0, :, -1, :]  # å–å‡ºæœ€åä¸€ä¸ªä½ç½® [num_heads, src_len]
        attention_maps.append(cross_attn.mean(dim=0).detach().cpu())  # å¹³å‡å¤šä¸ªå¤´ â†’ [src_len]

        if next_token == tokenizer_tgt.eos_id():
            break

    # ------------------ Step 5: è§£ç è¾“å‡ºä¸­æ–‡ ------------------
    output_ids = output_tokens[:-1]  # å»æ‰ <eos>
    translation = tokenizer_tgt.decode(output_ids)
    print("ğŸŒ è‹±æ–‡è¾“å…¥:", text)
    print("ğŸ‡¨ğŸ‡³ ç¿»è¯‘è¾“å‡º:", translation)

    # ------------------ Step 6: å¯è§†åŒ–äº¤å‰æ³¨æ„åŠ› ------------------
    plt.rcParams['font.family'] = 'SimHei'
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³åæ ‡è½´è´Ÿå·æ˜¾ç¤ºé—®é¢˜

    attn_matrix = torch.stack(attention_maps, dim=0)  # [tgt_len, src_len]

    src_tokens = ['<bos>'] + tokenizer_src.sp.encode(text, out_type=str) + ['<eos>']  # è¾“å…¥è‹±æ–‡ï¼ˆåˆ—æ ‡ç­¾ï¼‰
    tgt_tokens = tokenizer_tgt.sp.encode(translation, out_type=str) + ['<eos>']  # è¾“å‡ºä¸­æ–‡ï¼ˆè¡Œæ ‡ç­¾ï¼‰

    # plt.figure(figsize=(len(tgt_tokens) * 0.6 + 3, len(src_tokens) * 0.3 + 3))
    plt.figure(figsize=(16, 9))
    sns.heatmap(attn_matrix.numpy(),
                xticklabels=src_tokens,
                yticklabels=tgt_tokens,
                cmap='viridis', linewidths=0.5, annot=False, cbar=False)
    plt.xticks(rotation=0)  # æ¨ªå‘æ˜¾ç¤ºè¾“å…¥ token
    plt.yticks(rotation=0)  # è¡Œæœ¬æ¥ä¹Ÿæ¨ªå‘æ˜¾ç¤ºï¼Œè¿™é‡Œä¿ç•™æ˜ç¡®æŒ‡ç¤º
    plt.xlabel("è¾“å…¥è‹±æ–‡ Token")
    plt.ylabel("ç”Ÿæˆä¸­æ–‡ Token")
    plt.title(f"åŸæ–‡ï¼š {text} \n ç¿»è¯‘ï¼š{translation}")
    plt.tight_layout()
    plt.savefig("attention_map.png", dpi=300)
    plt.show()


if __name__ == '__main__':
    main()
